# replica-node-affinity.yaml

#Description: Check if the pods are scheduled on the affined node only.

###############################################################################################
#Test Steps:
#1. Check if the OpenEBS components are deployed
#2. Obtain the storage class yaml from the deployment.
#3. Modify the replica count in storageclass yaml and create it.
#4. Deploy Percona application.
#5. Check if the application and the volume pods are up and running.
#6. Update the node names in the patch yaml.
#7. Patch the deployment to include node affinity spec.
#8. Delete the replica pod in one of the nodes and check if it gets scheduled on the same node again.
#9. Delete the test artifacts.
###############################################################################################


- hosts: kubernetes-kubemasters

#Deploying openebs using helm chart.
  #roles:
  #  - {role: k8s-openebs-operator-helm}

  vars_files:
    - replica-node-affinity-vars.yaml

  tasks:

   - block:

# Checking if OpenEBS components are integrated successfully

       - name: 1) Check whether maya-apiserver pod is deployed
         shell: source ~/.profile; kubectl get pods --all-namespaces | grep maya-apiserver
         args:
           executable: /bin/bash
         register: result
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'Running' in result.stdout"
         delay: 30
         retries: 5

       - name: 1a) Get $HOME of K8s master for kubernetes user
         shell: source ~/.profile; echo $HOME
         args:
           executable: /bin/bash
         register: result_kube_home

       - name: 2) Obtaining storage classes yaml
         shell: kubectl get sc openebs-percona -o yaml > "{{ create_sc }}"
         args:
           executable: /bin/bash

# Updating number of replica pods to 2

       - name: 3) Replace the replica count in storage classes yaml
         replace:
           path: "{{ create_sc }}"
           regexp: 'openebs.io/jiva-replica-count: "3"'
           replace: 'openebs.io/jiva-replica-count: "2"'

       - name: 3a) Delete the existing storage class and create new one
         shell: kubectl delete sc openebs-percona; kubectl apply -f "{{ create_sc }}"
         args:
           executable: /bin/bash
         register: sc_out
         until: "'created' in sc_out.stdout"
         delay: 10
         retries: 5

       - name: 4) Deploying Percona Application
         command: kubectl create -f "{{ percona_link }}"
         args:
           executable: /bin/bash
         register: app_out
         until: "'created' in app_out.stdout"
         delay: 20
         retries: 6

       - name: 5) Confirm if the percona pod is running
         shell: source ~/.profile; kubectl get pods --all-namespaces | grep percona
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         register: result
         until: "'percona' and 'Running' in result.stdout"
         delay: 120
         retries: 10

       - name: 5a) Check if the replica pods are created
         shell: kubectl get pods --all-namespaces | grep rep | grep -i running |wc -l
         args:
           executable: /bin/bash
         register: rep_count
         until: "'2' in rep_count.stdout"
         delay: 60
         retries: 5
         ignore_errors: true

    # Getting PV name
       - name: 5b) Obtain PV name
         shell: kubectl get pv | grep openebs-percona | awk '{print $1}'
         args:
           executable: /bin/bash
         register: pv_name

       - name: 5c) Copy replica_patch.yaml to master
         copy:
           src: "{{ patch_file }}"
           dest: "{{ result_kube_home.stdout }}"

       - name: 6) Get the first node name where replica1 is scheduled
         shell: kubectl get po --all-namespaces -o wide | grep "{{ pv_name.stdout }}" | grep rep | awk '{print $8}' | awk 'FNR == 1 {print}'
         args:
           executable: /bin/bash
         register: node1

       - name: 6a) Get the second node name where replica1 is scheduled
         shell: kubectl get po --all-namespaces -o wide | grep "{{ pv_name.stdout }}" | grep rep | awk '{print $8}' | awk 'FNR == 2 {print}'
         args:
           executable: /bin/bash
         register: node2

       - name: 6b) Update first node name in replica_patch file
         replace:
           path: "{{ patch_file }}"
           regexp: 'nodename_where_replica_pod_1_got_scheduled'
           replace: '{{ node1.stdout }}'

       - name: 6c) Update second node name in replica_patch file
         replace:
           path: "{{ patch_file }}"
           regexp: 'nodename_where_replica_pod_2_got_scheduled'
           replace: '{{ node2.stdout }}'

       - name: 7) Obtain the deployment name
         shell: kubectl get deploy --all-namespaces |grep "{{ pv_name.stdout }}" | grep rep |awk '{print $2}'
         args:
           executable: /bin/bash
         register: deploy_name

       - name: 7a) Update the replica deployment with node affinity property
         shell: kubectl patch deployment "{{ deploy_name.stdout }}" -p "$(cat replica_patch.yaml)"
         args:
           executable: /bin/bash
         register: patch_out
         until: "'patched' in patch_out.stdout"
         delay: 20
         retries: 5

       - name: 8) Get pod name
         shell: kubectl get pods --all-namespaces -o wide | grep "{{ pv_name.stdout }}" | grep rep | grep "{{ node1.stdout }}" | awk '{print $2}'
         args:
           executable: /bin/bash
         register: pod_name

       - name: 8a) Delete the replica on one of the nodes
         shell: kubectl delete pod "{{ pod_name.stdout }}" --grace-period=0 --force
         args:
           executable: /bin/bash

       - name: 8b) Check the available replicas in deployment
         shell: kubectl get deploy --all-namespaces | grep "{{ pv_name.stdout }}" | grep rep | awk '{print $6}'
         args:
           executable: /bin/bash
         register: available_pods
         until: "'2' in available_pods.stdout"
         delay: 30
         retries: 5

       - name: 8c) Check if the replica is scheduled again
         shell: kubectl get pods --all-namespaces -o wide | grep "{{ pv_name.stdout }}" | grep rep | grep Running | grep "{{ node1.stdout }}"
         args:
           executable: /bin/bash
         register: result
         until: "'Running' in result.stdout"
         delay: 30
         retries: 15

       - name: setting flag if pass
         set_fact:
            flag: "Pass"

   - rescue:
       - name: setting flag if fail
         set_fact:
           flag: "Fail"

     always:

       - name: 9) Cleaning up the test artifacts
         include: replica-node-affinity-cleanup.yaml

       - name: Send slack notification
         slack:
           token: "{{ lookup('env','SLACK_TOKEN') }}"
           msg: '{{ ansible_date_time.time }} TEST: {{test_name}}, RESULT: {{ flag }}'
         when: slack_notify | bool and lookup('env','SLACK_TOKEN')















