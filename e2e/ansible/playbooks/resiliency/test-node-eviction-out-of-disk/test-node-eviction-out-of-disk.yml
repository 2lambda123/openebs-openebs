# test-node-eviction-out-of-disk-vars.yml
# Author: Sudarshan Darga
# Description: Verify volume replica pods patched with NodeAffinity are not evicted by Kubernetes (out-of-disk)
###############################################################################################
#Test Steps:
#1. Copy/Gather Test artifacts to Kubemaster.
#2. Deploy Percona application with one replica less than available nodes.
#3. Patch NodeAffinity to stick replica to a specific node.
#4. Configure Eviction taint on sticky node with node.kubernetes.io/out-of-disk condition.
#5. Verify evicted replica pod stays in 'Pending' state adhering NodeAffinity policy.
#6. Re-Configure/Remove Eviction taint on sticky node.
#7. Verify replica pod is scheduled and Running on the sticky node adhering NodeAffinity policy.
#8. Perform cleanup of test artifacts.
###############################################################################################

- hosts: localhost

  vars_files:
    - test-node-eviction-out-of-disk-vars.yml

  tasks:

   - block:

       - include: test-node-eviction-out-of-disk-prereq.yml

       ###################################################
       #              1)PREPARE FOR TEST                 #
       # (Place artifacts in kubemaster, start logger &  #
       # confirm OpenEBS operator is ready for requests. #
       ###################################################

       - include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/deploy_check.yml"
         vars:
            ns: "{{ operator_ns }}"
            app: maya-api

       - name: 1b) Copy test artifacts to master
         include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/copy_task.yml"
         vars:
           destination_node: "{{groups['kubernetes-kubemasters'].0}}"
           files_to_copy: 
             - "{{ storage_class_file }}"
             - "{{ percona_files }}"         

       - name: 1c) Get the Number of nodes count
         shell: source ~/.profile; kubectl get nodes | grep 'Ready' | grep 'none' | wc -l
         args:
           executable: /bin/bash
         register: node_out
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Fetch the node count from stdout
         set_fact:
            node_count: " {{ node_out.stdout}}"

       - name: 1d) Replace the replica count in storage classes yaml
         replace:
           path: "{{ storage_class_file }}"
           regexp: 'openebs.io/jiva-replica-count: "3"'
           replace: 'openebs.io/jiva-replica-count: "{{ (node_count) |int-1 }}"'
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       ####################################################
       #           2) Deploy Application                  #
       # (Apply replica patch and set NodeAffinity and    #
       # verify replica pods are re-scheduled and running)#
       ####################################################

       - name: 2a) Create test specific storage class
         shell: source ~/.profile; kubectl apply -f "{{ storage_class_file }}"
         args:
           executable: /bin/bash
         register: sc_out
<<<<<<< HEAD
         until: "'configured' in sc_out.stdout"
=======
         until: "'created' or 'configured' in sc_out.stdout"
>>>>>>> 3a7448fe5e141b4761d9e5f788cecb77abca4caf
         delay: 10
         retries: 5
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Create nodeeviction namespace
         shell: source ~/.profile; kubectl create ns {{ namespace }}
         args:
           executable: /bin/bash
         register: ns_out
         until: "'created' in ns_out.stdout"
         delay: 10
         retries: 5
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/deploy_task.yml"
         vars:
           app_yml: "{{ percona_files.0 }}"
           ns: "{{ namespace }}"

       - include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/deploy_check.yml"
         vars:
            ns: "{{ namespace }}"
            app: percona

       - name: 2c) Check specified number of replica pods are created
         shell: source ~/.profile; kubectl get pods -n {{ namespace }} | grep rep | grep -i running |wc -l
         args:
           executable: /bin/bash
         register: rep_count
         until: "'{{ (node_count) |int-1 }}' in rep_count.stdout"
         delay: 30
         retries: 15
         ignore_errors: true
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Get pv name associated with percona application
         shell: source ~/.profile; kubectl get pv | grep openebs-percona | awk '{print $1}'
         args:
           executable: /bin/bash
         register: pv_name
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/copy_task.yml"
         vars:
           destination_node: "{{groups['kubernetes-kubemasters'].0}}"
           files_to_copy: "{{ patch_file }}"

       - name: 2e) Gather node details for scheduled replicas
         shell: source ~/.profile; kubectl get pod -n {{ namespace }} -o wide | grep "{{ pv_name.stdout }}" | grep rep | awk '{print $7}'
         args:
           executable: /bin/bash
         register: node_details
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/regex_task.yml"
         vars:
           path: "{{ result_kube_home.stdout }}/{{ patch_file }}"
           regex1: "{{replace_item}}"
           regex2: "{{replace_with}}"

       - name: 3a) Get deployment details to set node affinity
         shell: source ~/.profile; kubectl get deploy -n {{ namespace }} |grep "{{ pv_name.stdout }}" | grep rep |awk '{print $1}'
         args:
           executable: /bin/bash
         register: deploy_name
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Wait for 120s before applying patch
         wait_for:
           timeout: 120

       - name: 3b) Set replica deployment with node affinity policy
<<<<<<< HEAD
         shell: kubectl patch deployment "{{ deploy_name.stdout }}" -n {{ namespace }} -p "$(cat {{ result_kube_home.stdout }}/replica_patch.yaml)"
=======
         shell: source ~/.profile; kubectl patch deployment "{{ deploy_name.stdout }}" -n {{ namespace }} -p "$(cat replica_patch.yaml)"
>>>>>>> 3a7448fe5e141b4761d9e5f788cecb77abca4caf
         args:
           executable: /bin/bash
         register: patch_out
         until: "'patched' in patch_out.stdout"
         delay: 20
         retries: 5
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 3c) Check whether replica pods are re-scheduled and running
         shell: source ~/.profile; kubectl get pods -n {{ namespace }} | grep rep | grep -i running |wc -l
         args:
           executable: /bin/bash
         register: rep_count
         until: "'2' in rep_count.stdout"
         delay: 30
         retries: 15
         ignore_errors: true
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       ####################################################
       # Set out-of-disk eviction taint condition on one  #
       # of the node where replica is scheduled.          #
       #(Verify replica pod adhering NodeAffinity Policy) #
       ####################################################

       - name: 4a) Get node on which replica is scheduled
         shell: source ~/.profile; kubectl get pods -n {{ namespace }} -o wide | grep rep | awk 'FNR == 1 {print}' | awk {'print $7'}
         args:
           executable: /bin/bash
         register: node_name
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 4b) Taint node with Eviction Condition node.kubernetes.io/out-of-disk
         shell: source ~/.profile; kubectl taint nodes {{ node_name.stdout }} {{ key }}=:NoExecute
         args:
           executable: /bin/bash
         register: taint_output
         until: "'tainted' in taint_output.stdout"
         delay: 30
         retries: 15
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 4c) Verify replica pod remains in Pending state
         shell: source ~/.profile; kubectl get pods -n {{ namespace }} | grep pvc | grep -i Pending |wc -l
         args:
           executable: /bin/bash
         register: check_replica
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'1' in check_replica.stdout"
         delay: 30
         retries: 15

       ####################################################
       # Remove out-of-disk eviction taint condition from #
       # the node.                                        #
       #(Verify replica pod is re-scheduled and Running   #
       # on same node adhering NodeAffinity Policy)       #
       ####################################################

       - name: 5a) Get pod_name to Verify replica pod is scheduled on same node
         shell: source ~/.profile; kubectl get pods -n {{ namespace }} | grep pvc | grep -i Pending | awk {'print $1'}
         args:
           executable: /bin/bash
         register: pod_name
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 5b) Untaint node with Eviction Condition node.kubernetes.io/out-of-disk
         shell: source ~/.profile; kubectl taint nodes {{ node_name.stdout }} {{ key }}:NoExecute-
         args:
           executable: /bin/bash
         register: untaint_output
         until: "'untainted' in untaint_output.stdout"
         delay: 30
         retries: 15
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: 5c) Verify replica pod is scheduled on same node adhering NodeAffinity Policy
         shell: source ~/.profile; kubectl get pods -n {{ namespace }} -o wide | grep {{ node_name.stdout }} | grep {{ pod_name.stdout }}
         args:
           executable: /bin/bash
         register: replica_status
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         until: "'Running' in replica_status.stdout"
         delay: 30
         retries: 5

       - name: Test Passed
         set_fact:
           flag: "Test Passed"

     rescue:
       - name: Test Failed
         set_fact:
           flag: "Test Failed"

     always:
       - block:

           - include: test-node-eviction-out-of-disk-cleanup.yml

           - name: Test Cleanup Passed
             set_fact:
               cflag: "Cleanup Passed"

         rescue:
           - name: Test Cleanup Failed
             set_fact:
               cflag: "Cleanup Failed"

         always:

           - include_tasks: "{{ansible_env.HOME}}/{{utils_path}}/stern_task.yml"
             vars:
               status: stop

           - name: Send slack notification
             slack:
               token: "{{ lookup('env','SLACK_TOKEN') }}"
               msg: '{{ ansible_date_time.time }} TEST: {{test_name}}, RESULT: {{ flag }},{{ cflag }}'
             when: slack_notify | bool and lookup('env','SLACK_TOKEN')

