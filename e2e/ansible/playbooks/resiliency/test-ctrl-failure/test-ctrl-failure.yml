# test-ctrl-failure.yml
# Author: Karthik
# Description: Include resiliency test suite in OpenEBS e2e (Test: periodic controller/target pod failures)
###############################################################################################
#Test Steps:
#1. Copy Test artifacts to Kubemaster.
#2. Deploy Percona application with liveness probe running db queries continuously.
#3. Create chaoskube infrastructe to induce failures.
#4. Gather Node,Container/target and chaoskube pod details required for test.
#5. Initiate periodic pod failures from chaoskube
#6. Check percona application status to ensure that it is running after the failure.
#7. Perform cleanup of test artifacts.
###############################################################################################

- hosts: localhost

  vars_files:
    - test-ctrl-failure-vars.yml

  tasks:
   - block:

       - include: test-ctrl-failure-prereq.yml

       ###################################################
       #                PREPARE FOR TEST                 #
       # (Place artifacts in kubemaster, start logger &  #
       # confirm OpenEBS operator is ready for requests. #
       ###################################################

       - name: 1) Get percona spec and liveness scripts
         get_url:
           url: "{{ item }}"
           dest: "{{ result_kube_home.stdout }}"
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         with_items: "{{ percona_links }}"

       - name: 1b) Replace volume-claim name with test parameters
         include_tasks: "{{utils_path}}/regex_task.yml"
         vars:
           path: "{{ result_kube_home.stdout }}/percona.yaml"
           regex1: "{{replace_item}}"
           regex2: "{{replace_with}}"

       - name: 1c) Copy the chaoskube specs to kubemaster
         include_tasks: "{{utils_path}}/copy_task.yml"
         vars:
           destination_node: "{{groups['kubernetes-kubemasters'].0}}"
           files_to_copy: "{{ chaoskube_files }}"

       - name: 1d) Replace volume-claim name with test parameters
         replace:
           path: "{{ result_kube_home.stdout }}/rbac.yaml"
           regexp: 'namespace: default'
           replace: 'namespace: {{ namespace }}'
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - include_tasks: "{{utils_path}}/deploy_check.yml"
         vars:
           ns: openebs
           lkey: name
           lvalue: maya-apiserver

       ####################################################
       #          SETUP FAULT-INJECTION ENV               #
       # (Setup chaoskube deployment with an empty policy,#
       # deploy percona w/ a liveness check for DB writes)#
       ####################################################

       - name: 2) Create test specific namespace
         include_tasks: "{{utils_path}}/namespace_task.yml"
         vars:
            status: create
            ns: "{{ namespace }}"

       - include_tasks: "{{utils_path}}/deploy_task.yml"
         vars:
           app_yml: "{{ chaoskube_files }}"
           ns: "{{ namespace }}"

       - name: 2a) Check whether chaoskube infrastructure is created
         include_tasks: "{{utils_path}}/deploy_check.yml"
         vars:
            ns: "{{ namespace }}"
            lkey: app
            lvalue: chaoskube

       - name: 2b) Set chaoskube pod name to variable
         set_fact:
           chaospod: "{{ result.stdout_lines[1].split()[0] }}"

       - name: 2c) Create a configmap with the liveness sql script
         shell: source ~/.profile; kubectl create configmap sqltest --from-file={{result_kube_home.stdout}}/{{ percona_files.1 }} -n {{ namespace }}
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         register: result
         failed_when: "'configmap' and 'created' not in result.stdout"

       - name: 3) Replace storage-class in app yaml to use jiva based storage engine
         replace:
           path: "{{ result_kube_home.stdout }}/{{ percona_files.0 }}"
           regexp: 'openebs-standard'
           replace: '{{ jiva_sc }}'
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         when: storage_engine  == 'jiva'

       - name: 3a) Replace storage-class in app yaml to use cstor based storage engine
         replace:
           path: "{{ result_kube_home.stdout }}/{{ percona_files.0 }}"
           regexp: 'openebs-standard'
           replace: '{{ cstor_sc }}'
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         when:  storage_engine  == 'cStor'

       - include_tasks: "{{utils_path}}/deploy_task.yml"
         vars:
           app_yml: "{{ percona_files.0 }}"
           ns: "{{ namespace }}"

       - include_tasks: "{{utils_path}}/deploy_check.yml"
         vars:
            ns: "{{ namespace }}"
            lkey: name
            lvalue: percona
         when:  storage_engine  == 'jiva'

       - name: 3b) Check if the cStor target has been created and running.
         shell: source ~/.profile; kubectl get pods -n {{ operator_ns }} | grep {{ namespace }}
         args:
           executable: /bin/bash
         register: cstor_pod
         until: "'Running' in cstor_pod.stdout"
         delay: 30
         retries: 15
         when: storage_engine  == 'cStor'

       - name: Wait for 120s to ensure liveness check starts
         wait_for:
           timeout: 120

       - block:

           - name: 4) Get the name of the volume ctrl deployment
             shell: >
               source ~/.profile; kubectl get deployments -n {{ namespace }}
               -l openebs.io/controller=jiva-controller --no-headers
             args:
               executable: /bin/bash
             delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
             register: result_deploy

           - name: 4a) Set the ctrl deployment name to variable
             set_fact:
               ctrl_deploy: "{{ result_deploy.stdout_lines[0].split()[0] }}"


       ########################################################
       #        INJECT FAULTS FOR SPECIFIED PERIOD            #
       # (Obtain begin marker before fault-inject(FI),do ctrl #
       # failures, verify successful FI via end marker)       #
       ########################################################

           - name: 4b) Get the resourceVersion of the controller deployment
             shell: >
               source ~/.profile; kubectl get deploy -n {{ namespace }}
               {{ ctrl_deploy }} -o yaml | grep resourceVersion
               | awk '{print $2}' | sed 's|"||g'
             args:
               executable: /bin/bash
             delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
             register: rv_bef

           - name: 5) Initiate periodic controller failures from chaoskube
             shell: >
               source ~/.profile; kubectl exec {{ chaospod }} -n {{ namespace }}
               -- timeout -t {{ chaos_duration }} chaoskube
               --labels 'openebs.io/controller=jiva-controller'
               --no-dry-run --interval={{ chaos_interval }}s
               --debug
               --namespaces={{ namespace }}
             args:
               executable: /bin/bash
             delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
             register: chaos_result
             ignore_errors: true

           - name: 5a) Get the resourceVersion of the controller deployment
             shell: >
               source ~/.profile; kubectl get deploy -n {{ namespace }}
               {{ ctrl_deploy }} -o yaml | grep resourceVersion
               | awk '{print $2}' | sed 's|"||g'
             args:
               executable: /bin/bash
             delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
             register: rv_aft

           - name: 5b) Compare resourceVersions of controller deployments
             debug:
               msg: "Verified controller pods were restarted by chaoskube"
             failed_when: "rv_bef.stdout | int == rv_aft.stdout | int"

       ########################################################
       #        VERIFY RESILINCY/FAULT-TOLERATION             #
       # (Confirm liveness checks on percona are successful & #
       # pod is still in running state)                       #
       ########################################################

           - include_tasks: "{{utils_path}}/deploy_check.yml"
             vars:
               ns: "{{ namespace }}"
               lkey: name
               lvalue: percona

         when: storage_engine  == 'jiva'

       - block:

           - name: 4) Get the name of the volume ctrl deployment
             shell: >
               source ~/.profile; kubectl get deployments -n {{ operator_ns }}
               -l openebs.io/target=cstor-target --no-headers
             args:
               executable: /bin/bash
             delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
             register: result_deploy

           - name: 4a) Set the target deployment name to variable
             set_fact:
               ctrl_deploy: "{{ result_deploy.stdout_lines[0].split()[0] }}"

       ########################################################
       #        INJECT FAULTS FOR SPECIFIED PERIOD            #
       # (Obtain begin marker before fault-inject(FI),do ctrl #
       # failures, verify successful FI via end marker)       #
       ########################################################

           - name: 4b) Get the resourceVersion of the controller deployment
             shell: >
               source ~/.profile; kubectl get deploy -n {{ operator_ns }}
               {{ ctrl_deploy }} -o yaml | grep resourceVersion
               | awk '{print $2}' | sed 's|"||g'
             args:
               executable: /bin/bash
             delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
             register: rv_bef

           - name: 5) Initiate periodic controller failures from chaoskube
             shell: >
               source ~/.profile; kubectl exec {{ chaospod }} -n {{ namespace }}
               -- timeout -t {{ chaos_duration }} chaoskube
               --labels 'openebs.io/target=cstor-target'
               --no-dry-run --interval={{ chaos_interval }}s
               --debug
               --namespaces={{ operator_ns }}
             args:
               executable: /bin/bash
             delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
             register: chaos_result
             ignore_errors: true

           - name: 5a) Get the resourceVersion of the controller deployment
             shell: >
               source ~/.profile; kubectl get deploy -n {{ operator_ns }}
               {{ ctrl_deploy }} -o yaml | grep resourceVersion
               | awk '{print $2}' | sed 's|"||g'
             args:
               executable: /bin/bash
             delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
             register: rv_aft

           - name: 5b) Compare resourceVersions of controller deployments
             debug:
               msg: "Verified controller pods were restarted by chaoskube"
             failed_when: "rv_bef.stdout | int == rv_aft.stdout | int"

       ########################################################
       #        VERIFY RESILINCY/FAULT-TOLERATION             #
       # (Confirm liveness checks on percona are successful & #
       # pod is still in running state)                       #
       ########################################################

           - name: 6) Check if the application is running
             shell: source ~/.profile; kubectl get pods -n {{ namespace }} -l name=percona --no-headers
             args:
               executable: /bin/bash
             register: app_pod
             until: "'Running' in app_pod.stdout"
             delay: 30
             retries: 15
          
         when: storage_engine  == 'cStor'


       ########################################################
       #                        CLEANUP                       #
       # (Tear down application, liveness configmap as well as#
       # the FI (chaoskube) infrastructure. Also stop logger) #
       ########################################################


       - name: Setting pass flag
         set_fact:
            flag: "Test Passed"
            status: "good"
            status_id: 1

     rescue:
       - name: Setting fail flag
         set_fact:
           flag: "Test Failed"
           status: "danger"
           status_id: 5

     always:
       - block:

           - include: test-ctrl-failure-cleanup.yml

           - name: Test Cleanup Passed
             set_fact:
               cflag: "Cleanup Passed"

         rescue:
           - name: Test Cleanup Failed
             set_fact:
               cflag: "Cleanup Failed"

         always:

           - include_tasks: "{{utils_path}}/stern_task.yml"
             vars:
               status: stop

           - include_tasks: "{{utils_path}}/namespace_task.yml"
             vars:
               status: delete
               ns: "{{ namespace }}"

           - name: Set Test Name as Fact
             set_fact:
               testname: "{{ test_name }}"
