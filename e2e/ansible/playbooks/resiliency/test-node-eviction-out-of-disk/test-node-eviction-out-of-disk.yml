# test-node-eviction-out-of-disk-vars.yml
# Author: Sudarshan Darga
# Description: Verify volume replica pods patched with NodeAffinity are not evicted by Kubernetes (out-of-disk)
###############################################################################################
#Test Steps:
#1. Copy/Gather Test artifacts to Kubemaster.
#2. Deploy Percona application with liveness probe.
#3. Once the application is up and running.
#4. Apply eviction taint node.kubernetes.io/out-of-disk on the node on which application pod is scheduled.
#5. Verify that application pod is up and running after it gets scheduled on another node.
#6. Perform cleanup of test artifacts.
#Note: Cleanup of removing taint and deleting namespace will be done, even if the test fails at any task.
##########################################################################################################

- hosts: localhost

  vars_files:
    - test-node-eviction-out-of-disk-vars.yml

  tasks:

   - block:

       - include: test-node-eviction-out-of-disk-prereq.yml

       ###################################################
       #              1)PREPARE FOR TEST                 #
       # (Place artifacts in kubemaster, start logger &  #
       # confirm OpenEBS operator is ready for requests. #
       ###################################################

       - include_tasks: "{{utils_path}}/deploy_check.yml"
         vars:
            ns: "{{ operator_ns }}"
            lkey: name
            lvalue: maya-apiserver

       - name: Get percona spec and liveness scripts
         get_url:
           url: "{{ item }}"
           dest: "{{ result_kube_home.stdout }}"
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         with_items: "{{ percona_links }}"

       - name: Replace volume-claim name with test parameters
         include_tasks: "{{utils_path}}/regex_task.yml"
         vars:
           path: "{{ result_kube_home.stdout }}/percona.yaml"
           regex1: "{{replace_item}}"
           regex2: "{{replace_with}}"
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"

       - name: Replace storage-class to use cstor storage engine
         replace:
           path: "{{ result_kube_home.stdout }}/percona.yaml"
           regexp: 'openebs-standard'
           replace: '{{ cstor_sc }}'
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         when:  storage_engine  == 'cStor'

       - name: Replace storage-class to use jiva storage engine
         replace:
           path: "{{ result_kube_home.stdout }}/percona.yaml"
           regexp: 'openebs-standard'
           replace: '{{ jiva_sc }}'
         delegate_to: "{{ groups['kubernetes-kubemasters'].0 }}"
         when:  storage_engine  == 'jiva'

       ####################################################
       #           2) Deploy Application                  #
       # (Apply replica patch and set NodeAffinity and    #
       # verify replica pods are re-scheduled and running)#
       ####################################################

       - name: Create nodeeviction namespace
         include_tasks: "{{utils_path}}/namespace_task.yml"
         vars:
            status: create
            ns: "{{ namespace }}"

       - name: Create a configmap with the liveness sql script
         shell: source ~/.profile; kubectl create configmap sqltest --from-file={{ result_kube_home.stdout }}/{{ percona_files.1 }} -n {{ namespace }}
         args:
           executable: /bin/bash
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         register: result
         failed_when: "'configmap' and 'created' not in result.stdout"

       - include_tasks: "{{utils_path}}/deploy_task.yml"
         vars:
           app_yml: "{{ percona_files.0 }}"
           ns: "{{ namespace }}"

       - include_tasks: "{{utils_path}}/deploy_check.yml"
         vars:
            ns: "{{ namespace }}"
            lkey: name
            lvalue: percona

       ####################################################
       # Set out-of-disk eviction taint condition on one  #
       # of the node where replica is scheduled.          #
       #(Verify replica pod adhering NodeAffinity Policy) #
       ####################################################

       - name: Wait for 120s to ensure liveness check to begin
         wait_for:
           timeout: 120

       - name:  Get node on which application pod is scheduled
         shell: source ~/.profile; kubectl get pods -n {{ namespace }} -o wide -l name=percona --no-headers -o custom-columns=:spec.nodeName
         args:
           executable: /bin/bash
         register: node_name
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Get replica which is scheduled on the node which will be tainted
         shell: source ~/.profile; kubectl get pods -n {{ namespace }} -o wide | grep {{ node_name.stdout }} | grep rep | awk {'print $1'}
         args:
           executable: /bin/bash
         register: rep_name
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
         when: storage_engine == 'jiva'

       - name:  Taint node with Eviction Condition node.kubernetes.io/out-of-disk
         shell: source ~/.profile; kubectl taint nodes {{ node_name.stdout }} {{ key }}=:NoExecute
         args:
           executable: /bin/bash
         register: taint_output
         until: "'tainted' in taint_output.stdout"
         delay: 30
         retries: 15
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       ####################################################
       # Remove out-of-disk eviction taint condition from #
       # the node.                                        #
       #(Verify replica pod is re-scheduled and Running   #
       # on same node adhering NodeAffinity Policy)       #
       ####################################################

       - name: Wait for 120s to ensure application pod is rescheduled and running.
         wait_for:
           timeout: 120

       - block:
           - name: Verify replica stikyness to the node
             shell: source ~/.profile; kubectl get pods -n {{ namespace }} -o wide | grep {{ node_name.stdout }} | grep {{ rep_name.stdout }}
             args:
               executable: /bin/bash
             register: result
             delegate_to: "{{groups['kubernetes-kubemasters'].0}}"
             until: "'Running' in result.stdout"
             delay: 30
             retries: 3

         when: storage_engine == 'jiva'

       - include_tasks: "{{utils_path}}/deploy_check.yml"
         vars:
            ns: "{{ namespace }}"
            lkey: name
            lvalue: percona

       - name: Untaint node with Eviction Condition node.kubernetes.io/out-of-disk
         shell: source ~/.profile; kubectl taint nodes {{ node_name.stdout }} {{ key }}:NoExecute-
         args:
           executable: /bin/bash
         register: untaint_output
         delegate_to: "{{groups['kubernetes-kubemasters'].0}}"

       - name: Test Passed
         set_fact:
           flag: "Test Passed"
           status: "good"
           status_id: 1

     rescue:
       - name: Test Failed
         set_fact:
           flag: "Test Failed"
           status: "danger"
           status_id: 5

     always:
       - block:

           - include: test-node-eviction-out-of-disk-cleanup.yml

           - name: Test Cleanup Passed
             set_fact:
               cflag: "Cleanup Passed"

         rescue:
           - name: Test Cleanup Failed
             set_fact:
               cflag: "Cleanup Failed"

         always:

           - include_tasks: "{{utils_path}}/stern_task.yml"
             vars:
               status: stop

           - include_tasks: "{{utils_path}}/namespace_task.yml"
             vars:
               status: delete
               ns: "{{ namespace }}"

           - name: Set Test Name as Fact
             set_fact:
               testname: "{{ test_name }}"
